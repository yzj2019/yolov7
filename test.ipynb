{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ab662ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import cv2\n",
    "from torchvision import transforms\n",
    "import numpy as np\n",
    "from utils.datasets import letterbox\n",
    "from models.experimental import attempt_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfd4b844",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fusing layers... \n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "weights = './weights/yolov7-w6.pt'\n",
    "model = attempt_load(weights, map_location=device)\n",
    "_ = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ee054f1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Upsample' object has no attribute 'recompute_scale_factor'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32md:\\Yzj\\projects\\2d_detection_for_crane\\yolov7\\test.ipynb 单元格 3\u001b[0m in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W2sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m image \u001b[39m=\u001b[39m image\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W2sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# image = image.half()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W2sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m output \u001b[39m=\u001b[39m model(image)\n",
      "File \u001b[1;32md:\\Tools\\miniconda\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Yzj\\projects\\2d_detection_for_crane\\yolov7\\models\\yolo.py:599\u001b[0m, in \u001b[0;36mModel.forward\u001b[1;34m(self, x, augment, profile)\u001b[0m\n\u001b[0;32m    597\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(y, \u001b[39m1\u001b[39m), \u001b[39mNone\u001b[39;00m  \u001b[39m# augmented inference, train\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 599\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward_once(x, profile)\n",
      "File \u001b[1;32md:\\Yzj\\projects\\2d_detection_for_crane\\yolov7\\models\\yolo.py:625\u001b[0m, in \u001b[0;36mModel.forward_once\u001b[1;34m(self, x, profile)\u001b[0m\n\u001b[0;32m    622\u001b[0m         dt\u001b[39m.\u001b[39mappend((time_synchronized() \u001b[39m-\u001b[39m t) \u001b[39m*\u001b[39m \u001b[39m100\u001b[39m)\n\u001b[0;32m    623\u001b[0m         \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m%10.1f\u001b[39;00m\u001b[39m%10.0f\u001b[39;00m\u001b[39m%10.1f\u001b[39;00m\u001b[39mms \u001b[39m\u001b[39m%-40s\u001b[39;00m\u001b[39m'\u001b[39m \u001b[39m%\u001b[39m (o, m\u001b[39m.\u001b[39mnp, dt[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m], m\u001b[39m.\u001b[39mtype))\n\u001b[1;32m--> 625\u001b[0m     x \u001b[39m=\u001b[39m m(x)  \u001b[39m# run\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     y\u001b[39m.\u001b[39mappend(x \u001b[39mif\u001b[39;00m m\u001b[39m.\u001b[39mi \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msave \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m)  \u001b[39m# save output\u001b[39;00m\n\u001b[0;32m    629\u001b[0m \u001b[39mif\u001b[39;00m profile:\n",
      "File \u001b[1;32md:\\Tools\\miniconda\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\Tools\\miniconda\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\upsampling.py:157\u001b[0m, in \u001b[0;36mUpsample.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m    156\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39minterpolate(\u001b[39minput\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msize, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mscale_factor, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malign_corners,\n\u001b[1;32m--> 157\u001b[0m                          recompute_scale_factor\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrecompute_scale_factor)\n",
      "File \u001b[1;32md:\\Tools\\miniconda\\envs\\yolov7\\lib\\site-packages\\torch\\nn\\modules\\module.py:1265\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1263\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[0;32m   1264\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1265\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m   1266\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Upsample' object has no attribute 'recompute_scale_factor'"
     ]
    }
   ],
   "source": [
    "image = cv2.imread('./inference/images/horses.jpg')  # 504x378 image\n",
    "image = letterbox(image, 1280, stride=64, auto=True)[0]\n",
    "image_ = image.copy()\n",
    "image = transforms.ToTensor()(image)\n",
    "image = torch.tensor(np.array([image.numpy()]))\n",
    "image = image.to(device)\n",
    "image = image.half()\n",
    "\n",
    "output = model(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path\n",
    "from utils.plots import plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized, TracedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'opt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32md:\\Yzj\\projects\\2d_detection_for_crane\\yolov7\\test.ipynb 单元格 4\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m source, weights, view_img, save_txt, imgsz, trace \u001b[39m=\u001b[39m opt\u001b[39m.\u001b[39msource, opt\u001b[39m.\u001b[39mweights, opt\u001b[39m.\u001b[39mview_img, opt\u001b[39m.\u001b[39msave_txt, opt\u001b[39m.\u001b[39mimg_size, \u001b[39mnot\u001b[39;00m opt\u001b[39m.\u001b[39mno_trace\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m save_img \u001b[39m=\u001b[39m \u001b[39mnot\u001b[39;00m opt\u001b[39m.\u001b[39mnosave \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m source\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m)  \u001b[39m# save inference images\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m webcam \u001b[39m=\u001b[39m source\u001b[39m.\u001b[39misnumeric() \u001b[39mor\u001b[39;00m source\u001b[39m.\u001b[39mendswith(\u001b[39m'\u001b[39m\u001b[39m.txt\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mor\u001b[39;00m source\u001b[39m.\u001b[39mlower()\u001b[39m.\u001b[39mstartswith(\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/Yzj/projects/2d_detection_for_crane/yolov7/test.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m         (\u001b[39m'\u001b[39m\u001b[39mrtsp://\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrtmp://\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttp://\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mhttps://\u001b[39m\u001b[39m'\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'opt' is not defined"
     ]
    }
   ],
   "source": [
    "source, weights, view_img, save_txt, imgsz, trace = opt.source, opt.weights, opt.view_img, opt.save_txt, opt.img_size, not opt.no_trace\n",
    "save_img = not opt.nosave and not source.endswith('.txt')  # save inference images\n",
    "webcam = source.isnumeric() or source.endswith('.txt') or source.lower().startswith(\n",
    "        ('rtsp://', 'rtmp://', 'http://', 'https://'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536ecc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize\n",
    "# set_logging()\n",
    "device = select_device(opt.device)\n",
    "half = device.type != 'cpu'  # half precision only supported on CUDA\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check img_size\n",
    "\n",
    "if trace:\n",
    "    model = TracedModel(model, device, opt.img_size)\n",
    "\n",
    "if half:\n",
    "    model.half()  # to FP16\n",
    "\n",
    "# Second-stage classifier\n",
    "classify = False\n",
    "if classify:\n",
    "    modelc = load_classifier(name='resnet101', n=2)  # initialize\n",
    "    modelc.load_state_dict(torch.load('weights/resnet101.pt', map_location=device)['model']).to(device).eval()\n",
    "\n",
    "# Set Dataloader\n",
    "vid_path, vid_writer = None, None\n",
    "if webcam:\n",
    "    view_img = check_imshow()\n",
    "    cudnn.benchmark = True  # set True to speed up constant image size inference\n",
    "    dataset = LoadStreams(source, img_size=imgsz, stride=stride)\n",
    "else:\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "# Get names and colors\n",
    "names = model.module.names if hasattr(model, 'module') else model.names\n",
    "colors = [[random.randint(0, 255) for _ in range(3)] for _ in names]\n",
    "# Run inference\n",
    "if device.type != 'cpu':\n",
    "    model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "old_img_w = old_img_h = imgsz\n",
    "old_img_b = 1\n",
    "\n",
    "t0 = time.time()\n",
    "for path, img, im0s, vid_cap in dataset:\n",
    "    img = torch.from_numpy(img).to(device)\n",
    "    img = img.half() if half else img.float()  # uint8 to fp16/32\n",
    "    img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "    if img.ndimension() == 3:\n",
    "        img = img.unsqueeze(0)\n",
    "\n",
    "    # Warmup\n",
    "    if device.type != 'cpu' and (old_img_b != img.shape[0] or old_img_h != img.shape[2] or old_img_w != img.shape[3]):\n",
    "        old_img_b = img.shape[0]\n",
    "        old_img_h = img.shape[2]\n",
    "        old_img_w = img.shape[3]\n",
    "        for i in range(3):\n",
    "            model(img, augment=opt.augment)[0]\n",
    "\n",
    "    # Inference\n",
    "    t1 = time_synchronized()\n",
    "    with torch.no_grad():   # Calculating gradients would cause a GPU memory leak\n",
    "        pred = model(img, augment=opt.augment)[0]\n",
    "    t2 = time_synchronized()\n",
    "\n",
    "    # Apply NMS\n",
    "    pred = non_max_suppression(pred, opt.conf_thres, opt.iou_thres, classes=opt.classes, agnostic=opt.agnostic_nms)\n",
    "    t3 = time_synchronized()\n",
    "\n",
    "    # Apply Classifier\n",
    "    if classify:\n",
    "        pred = apply_classifier(pred, modelc, img, im0s)\n",
    "\n",
    "    # Process detections\n",
    "    for i, det in enumerate(pred):  # detections per image\n",
    "        if webcam:  # batch_size >= 1\n",
    "            p, s, im0, frame = path[i], '%g: ' % i, im0s[i].copy(), dataset.count\n",
    "        else:\n",
    "            p, s, im0, frame = path, '', im0s, getattr(dataset, 'frame', 0)\n",
    "\n",
    "        p = Path(p)  # to Path\n",
    "        save_path = str(save_dir / p.name)  # img.jpg\n",
    "        txt_path = str(save_dir / 'labels' / p.stem) + ('' if dataset.mode == 'image' else f'_{frame}')  # img.txt\n",
    "        gn = torch.tensor(im0.shape)[[1, 0, 1, 0]]  # normalization gain whwh\n",
    "        if len(det):\n",
    "            # Rescale boxes from img_size to im0 size\n",
    "            det[:, :4] = scale_coords(img.shape[2:], det[:, :4], im0.shape).round()\n",
    "\n",
    "            # Print results\n",
    "            for c in det[:, -1].unique():\n",
    "                n = (det[:, -1] == c).sum()  # detections per class\n",
    "                s += f\"{n} {names[int(c)]}{'s' * (n > 1)}, \"  # add to string\n",
    "\n",
    "            # Write results\n",
    "            for *xyxy, conf, cls in reversed(det):\n",
    "                if save_txt:  # Write to file\n",
    "                    xywh = (xyxy2xywh(torch.tensor(xyxy).view(1, 4)) / gn).view(-1).tolist()  # normalized xywh\n",
    "                    line = (cls, *xywh, conf) if opt.save_conf else (cls, *xywh)  # label format\n",
    "                    with open(txt_path + '.txt', 'a') as f:\n",
    "                        f.write(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "\n",
    "                if save_img or view_img:  # Add bbox to image\n",
    "                    label = f'{names[int(cls)]} {conf:.2f}'\n",
    "                    plot_one_box(xyxy, im0, label=label, color=colors[int(cls)], line_thickness=1)\n",
    "\n",
    "        # Print time (inference + NMS)\n",
    "        print(f'{s}Done. ({(1E3 * (t2 - t1)):.1f}ms) Inference, ({(1E3 * (t3 - t2)):.1f}ms) NMS')\n",
    "\n",
    "        # Stream results\n",
    "        if view_img:\n",
    "            cv2.imshow(str(p), im0)\n",
    "            cv2.waitKey(1)  # 1 millisecond\n",
    "\n",
    "        # Save results (image with detections)\n",
    "        if save_img:\n",
    "            if dataset.mode == 'image':\n",
    "                cv2.imwrite(save_path, im0)\n",
    "                print(f\" The image with the result is saved in: {save_path}\")\n",
    "            else:  # 'video' or 'stream'\n",
    "                if vid_path != save_path:  # new video\n",
    "                    vid_path = save_path\n",
    "                    if isinstance(vid_writer, cv2.VideoWriter):\n",
    "                        vid_writer.release()  # release previous video writer\n",
    "                    if vid_cap:  # video\n",
    "                        fps = vid_cap.get(cv2.CAP_PROP_FPS)\n",
    "                        w = int(vid_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "                        h = int(vid_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "                    else:  # stream\n",
    "                        fps, w, h = 30, im0.shape[1], im0.shape[0]\n",
    "                        save_path += '.mp4'\n",
    "                    vid_writer = cv2.VideoWriter(save_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (w, h))\n",
    "                vid_writer.write(im0)\n",
    "\n",
    "if save_txt or save_img:\n",
    "    s = f\"\\n{len(list(save_dir.glob('labels/*.txt')))} labels saved to {save_dir / 'labels'}\" if save_txt else ''\n",
    "    #print(f\"Results saved to {save_dir}{s}\")\n",
    "\n",
    "print(f'Done. ({time.time() - t0:.3f}s)')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('yolov7')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "59190a0bac088527b3a42be922a970ae7446e6b1d23038ff0e9d6d632fd72ea3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
